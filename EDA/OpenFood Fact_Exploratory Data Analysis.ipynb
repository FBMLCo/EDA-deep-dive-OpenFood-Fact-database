{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Analysis Plan\n",
    "* **A. Importing & cleaning the data**\n",
    "* **B. Manipulating the data: 3 questions**\n",
    "    * What are across countries products with the highest level of energy? and what are the top 10 for the main countries? What are the Top 20 Energy products within the 75% most common energy levels?\n",
    "    * What is the proportion of products with additives? Is there a pattern between high calory products and number of additives\n",
    "    * What are the products with the most balanced levels of core nutrients? What is their level of calory? What is their breakdown of core nutrients?\n",
    "* **C. Text data**\n",
    "* **D. Time series analysis**\n",
    "* **E. Buidling a database**\n",
    "* **F. Finding correlations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Importing & Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-102e0a4743ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing the file and creating a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mOFmaster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/fbaff/Documents/Data Science/EPFL/0_Datasets/foodfacts/en.openfoodfacts.org.products.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing the file and creating a dataframe\n",
    "OFmaster=pd.read_csv(\"C:/Users/fbaff/Documents/Data Science/EPFL/0_Datasets/foodfacts/en.openfoodfacts.org.products.tsv\",low_memory=False ,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OFmaster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data plan\n",
    "* Address NaN\n",
    "* Dealing with duplicates or incorrect values\n",
    "* Adapting data format\n",
    "* Detecting and addressing outliers\n",
    "* Reference of header descriptions at:https://world.openfoodfacts.org/data/data-fields.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the data structure\n",
    "OFmaster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the different types of data\n",
    "OFmaster.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating number of records per column\n",
    "OFmaster.notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adress NaN issues (principles)\n",
    "* Drop columns for which there is no data at all\n",
    "* Drop products (i.e rows) for which there are too many missing data for the variables supporting their unique identification;\n",
    "* i.e variables in the \"General information\" section: https://world.openfoodfacts.org/data/data-fields.txt\n",
    "* It will help identifying and addressing duplicates afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variables which do not have value at all (i.e Not null = 0)\n",
    "df1=OFmaster.dropna(how='all', axis=1)\n",
    "df1.shape # 356027 rows (no change); 16 columns have been removed (147 vs 163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining NaN values per column\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN for variable which are critical for identifying a product\n",
    "df2=df1.dropna(subset=[\"code\",\"creator\",\"created_datetime\",\"product_name\",\"ingredients_text\"])\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NaN for variables that we will integrate in the question \"Text Data\"\n",
    "df2[\"ingredients_text\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It remains 282069 records (out of 356027) which is an acceptable trade-off regarding the quality of the dataset\n",
    "# We may decide to include back the records with missing value related to ingredient text for increasing the size of the reference data\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of duplicates on a selection of variables\n",
    "* bar code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fce084000449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Check if there are duplicate bar codes (i.e variable \"code\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcodedup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcodedup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodedup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcodedup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# There is no duplicate in the column \"code\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if there are duplicate bar codes (i.e variable \"code\")\n",
    "codedup = df2[\"code\"]\n",
    "df2[codedup.isin(codedup[codedup.duplicated()])]\n",
    "# There is no duplicate in the column \"code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting data format\n",
    "* Transform dates variable related object into proper date format\n",
    "* Aligning the name of countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f3a5e56d7434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Change the following variables: created_datetime, last_modified_datetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_datetime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'last_modified_datetime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'last_modified_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Change the following variables: created_datetime, last_modified_datetime\n",
    "df2['created_datetime'] = pd.to_datetime(df2['created_datetime'])\n",
    "df2['last_modified_datetime'] = pd.to_datetime(df2['last_modified_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-aaa807e366d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Explore the list of countries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'countries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# Explore the list of countries\n",
    "df2['countries'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to align the name of the following countries:\n",
    "def newname(s):\n",
    "    s=s.str.replace('US','United States')\n",
    "    s=s.str.replace('Suisse','Switzerland')\n",
    "    s=s.str.replace('Deutschland','Germany')\n",
    "    s=s.str.replace('España','Spain')\n",
    "    s=s.str.replace('en:FR,France','France')\n",
    "    s=s.str.replace('Россия','Russia')\n",
    "    s=s.str.replace('en:CH','Switzerland')\n",
    "    s=s.str.replace('en:US','United States')\n",
    "    s=s.str.replace('Schweiz','Switzerland')\n",
    "    s=s.str.replace('en:FR','France')\n",
    "    s=s.str.replace('en:United States','United States')\n",
    "    s=s.str.replace('en:GB','United Kingdom')\n",
    "    return s\n",
    "# there are country names which would need to be changed. The current command address the vast majority of observations which\n",
    "# will be covered by the analysis afterwards on the Top countries (Manipulating data, question 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for the modifying the name of countries\n",
    "new_countries=df2['countries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the \"newname\" function to new countries\n",
    "new_countries=newname(new_countries)\n",
    "new_countries.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate the \"new-countries\" as a new column in df2\n",
    "df2['new_countries']=new_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column 'countries'\n",
    "df2.drop(['countries'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting and addressing outliers for the following variables:\n",
    "* Overall assumptions: if outliers are explained by some incorrect reported figures we will remove the product from the analysis\n",
    "* We will take 2 variables depending on a multiple factors to evaluate the integrity of the database:\n",
    "* energy_100g (267,005 obervations) - Conclusion:  35 identified outliers (Sigma>4). After a manual check of about 10 data points (in and out outliers), it seems that reported figures seemed to be correct. We keep these products.\n",
    "* cholesterol_100g (143,614 observations) - Conclusion 8 identified outliers with Sigma>3. We keep as it is very small number\n",
    "* Overall conclusion: we keep all products at this stage based on the analysis of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-77c0659fc2a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assess outliers for the variable energy_100g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2e9a293cf2f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# let's try to define the function which will help to identfy data with a std deviation >4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# energy_100g\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfilter0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'energy_100g'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'energy_100g'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'energy_100g'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfilter0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 35 products\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# let's try to define the function which will help to identfy data with a std deviation >4\n",
    "# energy_100g\n",
    "filter0 = np.abs(df2['energy_100g'] - df2['energy_100g'].mean()) > (4 * df2['energy_100g'].std())\n",
    "filter0.sum() # 35 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the list of outliers with an energy value >4 Sigma\n",
    "outliers=df2[filter0]\n",
    "outliers_check=outliers[['code', 'product_name', 'energy_100g']]\n",
    "outliers_check.sort_values(['energy_100g'], ascending=False).head()\n",
    "# after having checked about 10 products (reported figures vs other databases), it appears that the majority is correct.\n",
    "# We will keep all products (even if some mistakes in reporting the figures are likely to have happened for some inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assess outliers for the variable: cholesterol_100g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bd1186926396>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cholesterol_100g'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "df2['cholesterol_100g'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to define the function which will help to identfy data with a std deviation >3\n",
    "# energy_100g\n",
    "filter1 = np.abs(df2['cholesterol_100g'] - df2['cholesterol_100g'].mean()) > (3 * df2['cholesterol_100g'].std())\n",
    "filter1.sum() # 8 products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Manipulating the data\n",
    "* What are across countries products with the highest level of energy? and what are the top 10 for the main countries? What are the Top 20 Energy products within the 75% most common energy levels?\n",
    "* What is the proportion of products with additives? Is there a pattern between high calory products and number of additives?\n",
    "* What are the products with the most balanced levels of core nutrients? What is their level of calory? What is their breakdown of core nutrients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are across countries products with the highest level of energy? and what are the top 10 for the main countries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a summarized dataframe\n",
    "nrj=df2[['code','product_name','new_countries','energy_100g','proteins_100g','carbohydrates_100g','sugars_100g','fat_100g',\"sodium_100g\"]]\n",
    "nrj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nrj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6911b60f018e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnrj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nrj' is not defined"
     ]
    }
   ],
   "source": [
    "nrj.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN for energy_100g\n",
    "nrj_clean=nrj.dropna(subset=['energy_100g'])\n",
    "nrj_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrj_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nrj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-faa1ed2c62bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# sort energy_100g values (descending)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnrj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'energy_100g'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nrj' is not defined"
     ]
    }
   ],
   "source": [
    "# sort energy_100g values (descending)\n",
    "nrj.sort_values(['energy_100g'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nrj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5bc1c7e0562c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# check the countries with the highest number of observations: Top 3 US, France, Switzerland\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnrj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'new_countries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nrj' is not defined"
     ]
    }
   ],
   "source": [
    "# check the countries with the highest number of observations: Top 3 US, France, Switzerland\n",
    "nrj['new_countries'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 highest energy products listed in the US only\n",
    "top10US=nrj.loc[nrj.new_countries=='United States']\n",
    "top10US[['product_name','energy_100g','new_countries']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index to get the country names\n",
    "top10US.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data to plot\n",
    "v1US = top10US.sort_values(['energy_100g'], ascending=False)[0:10]['energy_100g']\n",
    "v1US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Top 10 products for the US\n",
    "bar_plotUS = v1US.plot.barh()\n",
    "bar_plotUS.set_title('US - Top 10 products with highest level of energy_100g (kj)', fontweight=\"bold\")\n",
    "bar_plotUS.set_xlabel('Level of calories in Kj') # x label\n",
    "bar_plotUS.invert_yaxis() # invert the y axis to get the highest difficulty first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key takeaways\n",
    "* Most energetic product reported in the US are transformed products with a lot of sugars/fat and/or concentrated products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 highest energy products listed in the France only\n",
    "top10FR=nrj.loc[nrj.new_countries=='France']\n",
    "top10FR[['product_name','energy_100g','new_countries']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index to get the country names\n",
    "top10FR.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for the plot\n",
    "v1FR=top10FR.sort_values(['energy_100g'], ascending=False)[0:10]['energy_100g']\n",
    "v1FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Top 10 products for France\n",
    "bar_plotFR = v1FR.plot.barh()\n",
    "bar_plotFR.set_title('FRANCE - Top 10 products with highest level of energy_100g (kj)', fontweight=\"bold\")\n",
    "bar_plotFR.set_xlabel('Level of calories in Kj') # x label\n",
    "bar_plotFR.invert_yaxis() # invert the y axis to get the highest difficulty first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key takeaways\n",
    "* The overall level of calories (Kj) for top energetic products reported in France are much lower that the most energetic products in the US\n",
    "* Top 3 level of calories is driven by fat, proteins and/or sugars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 highest energy products listed in the Switzerland only\n",
    "top10CH=nrj.loc[nrj.new_countries=='Switzerland']\n",
    "top10CH[['product_name','energy_100g','new_countries']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top10CH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-532c096788c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# change the index to get the product names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtop10CH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'product_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'top10CH' is not defined"
     ]
    }
   ],
   "source": [
    "# change the index to get the product names\n",
    "top10CH.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for the plot\n",
    "v1CH=top10CH.sort_values(['energy_100g'], ascending=False)[0:10]['energy_100g']\n",
    "v1CH\n",
    "# not relevant to plot CH data as they have all the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key takeaways\n",
    "* Majority of high level calory for prodducts reported in CH are heavy fat product being part almots of the same category\n",
    "* Interpretation is very limited in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Top 20 Energy products within the 75% most common energy levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which will help to identify data with a std deviation >3 (i.e outliers)\n",
    "# energy_100g\n",
    "filter0 = np.abs(nrj_clean['energy_100g'] - nrj_clean['energy_100g'].mean()) > (3 * nrj_clean['energy_100g'].std())\n",
    "filter0.sum() # 98 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select all the rows now considered as outliers\n",
    "outliers = nrj_clean.loc[filter0, :]\n",
    "outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new dataframe without outliers\n",
    "nrj_new = nrj_clean.drop(outliers.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrj_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the distribution plot of the energy variable\n",
    "sns.distplot(nrj_new.energy_100g)\n",
    "plt.rcParams[\"figure.figsize\"]=[20,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index to get the product names\n",
    "nrj_new.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select energy data with a value < 1674 (75% of all observations)\n",
    "nrj_new2=nrj_new[nrj_new['energy_100g'] < 1674]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 products with a calory level below 1674 Kj\n",
    "nrj3= nrj_new2.sort_values(['energy_100g'], ascending=False)[0:20]['energy_100g']\n",
    "nrj3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeaways\n",
    "* Top 20 products with a calory level (Kj) with the 75% most common energy levels\n",
    "######  1. Moelleux fourrés fraise\n",
    "######  2. 10 Pains au Chocolat\n",
    "######  3. Cheddar au poivre\n",
    "######  4. Smilly Fourrage à la Fraise   \n",
    "######  5. Crème de nougat blanc \n",
    "######  6. Moelleux fourré Fraise   \n",
    "######  7. Mélange de Biscuits - Recette Japonaise  \n",
    "######  8. Bonbons tendres aux goûts fruités   \n",
    "######  9. Schoko 30% weniger Zucker              \n",
    "######  10. Flora Original                           \n",
    "######  11. Ficelles de Pain Recette Sésame pavot   \n",
    "######  12. Flora original                            \n",
    "######  13. Barres riz et blé complet chocolat        \n",
    "######  14. Multi Frutti                              \n",
    "######  15. Snack poppé saveur paprika                 \n",
    "######  16. Schoko 30% weniger Zucker                 \n",
    "######  17. Pâté de foie pur porc                     \n",
    "######  18. Rosette                                   \n",
    "######  19. Pesto Verde                               \n",
    "######  20. Délice d'Amandes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the proportion of products with additives? Is there a pattern between high calory products and number of aditives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a summarized dataframe\n",
    "add_analysis=df2[['code','product_name','new_countries','energy_100g','additives_n','proteins_100g','carbohydrates_100g','sugars_100g','fat_100g',\"sodium_100g\"]]\n",
    "add_analysis.shape # 282,069 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN for energy_100g\n",
    "add_analysis_clean=add_analysis.dropna(subset=['energy_100g'])\n",
    "add_analysis_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select energy data with values > 0\n",
    "add_analysis_clean=add_analysis_clean[add_analysis_clean['energy_100g'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of products with additives\n",
    "a=add_analysis_clean['code'].count()\n",
    "b=add_analysis_clean[add_analysis_clean['additives_n']>=1].code.count()\n",
    "b/a*100 # 61.98% of product listed in the cleaned database include additives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the distribution plot of the nb of additives\n",
    "dist=add_analysis_clean[add_analysis_clean['additives_n']>=1]\n",
    "sns.distplot(dist.additives_n)\n",
    "plt.title('Graph 1: Distribution Plot - Number of Additives', fontweight=\"bold\")\n",
    "plt.rcParams[\"figure.figsize\"]=[20,10]\n",
    "# large majority of number of additives per product is <5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_analysis_clean['energy_100g'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_analysis_clean['additives_n'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot level of energy and number of additives (exploratory for energy values <10,000 Kj)\n",
    "sns.pairplot(add_analysis_clean,  x_vars = ['energy_100g'], y_vars=['additives_n'], size = 5)\n",
    "plt.title('Graph 2 - Level of Calories (Kj) vs Number of Additives',fontweight=\"bold\")\n",
    "plt.xlim([0, 10000])\n",
    "plt.rcParams[\"figure.figsize\"]=[20,10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeways\n",
    "* A large majority of number of additives per product is <5 (Graph 1) \n",
    "* Most of the products which have a calory level < 4000 Kj would have at least one additive (Graph 2)\n",
    "* It appears that there is not necessarily a correlation between the number of additives and a high level of calories or eventually the number of additives may have an impact of a low level of calories (to be further explored in the last part of the project); Graph 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the products with the most balanced levels of core nutrients? What is their level of calory? What is their breakdown of core nutrients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary remarks: What is a balanced level of nutrients?\n",
    "* According to common practice, the levell of carbohydrates should be ranged between 45% and 65% of total calories \n",
    "(https://www.livescience.com/51976-carbohydrates.html)\n",
    "* Additionally, WHO recommendations for adult are: less than 10% of free sugars and less than 30% of fat   \n",
    "(http://www.who.int/en/news-room/fact-sheets/detail/healthy-diet)\n",
    "* Even if those % are generally appplied to an average daily regime, we will try to identify products in the factfood database which would match these criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new dataframe food3 based on the dataframe from the previous question\n",
    "food3 =add_analysis_clean\n",
    "food3.shape # 257,802 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to compute the % of proteins, carbohydrates, fat for each given products and other calculus for visualization\n",
    "food3['carbohydrates_perc']=(food3['carbohydrates_100g']/food3['energy_100g'])*100\n",
    "food3['fat_perc']=(food3['fat_100g']/food3['energy_100g'])*100\n",
    "food3['sugars_perc']=(food3['sugars_100g']/food3['energy_100g'])*100\n",
    "food3['core_nutrients_perc']=food3['sugars_perc']+food3['carbohydrates_perc']+food3['fat_perc']\n",
    "food3['Others_perc']=100-food3['core_nutrients_perc'] # variable useful for stacked bar chart\n",
    "food3['Total_perc']=food3['core_nutrients_perc']+food3['Others_perc']\n",
    "food3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some inconsistencies and/or mistakes in the way figures are reported. As a consequence, we will consider products\n",
    "# with a Total_% of 100% and less for this specific question\n",
    "food4=food3[food3['core_nutrients_perc'] <= 100]\n",
    "food4.shape # 245,052 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the selection rules as: Carbohydrates (45% to 65%) and Fat (20%-35%) and Protein (10%-35%)\n",
    "target=food4[(food4['carbohydrates_perc'] >= 45) & (food4['carbohydrates_perc'] <= 65) & (food4['sugars_perc'] <=10) & (food4['fat_perc'] <=30)]\n",
    "target.shape # 29 products are matching these criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index to get the product names\n",
    "target.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for the plot (ranking based on the level of energy)\n",
    "targ1=target.sort_values(['energy_100g'], ascending=False)\n",
    "targ1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 29 products matching balance nutrients criteria (ranking based on their level of calories)\n",
    "bar_plot1 = targ1['energy_100g'].plot.barh()\n",
    "bar_plot1.set_title('Graph 3 - 29 most nutrient balanced products - Ranking based on their level of calories (kj)',fontweight=\"bold\")\n",
    "bar_plot1.set_xlabel('Level of calories in Kj') # x label\n",
    "plt.rcParams[\"figure.figsize\"]=[30,10]\n",
    "bar_plot1.invert_yaxis() # invert the y axis to get the highest difficulty first\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "others = targ1[0:10]['Others_perc']\n",
    "carbohydrates = targ1[0:10]['carbohydrates_perc']\n",
    "sugars = targ1[0:10]['sugars_perc']\n",
    "fat = targ1[0:10]['fat_perc']\n",
    "y=np.arange(len(others))\n",
    "\n",
    "# plot\n",
    "plt.barh(y, others,color='red')\n",
    "plt.barh(y,carbohydrates,color='gray', left=others)\n",
    "plt.barh(y,sugars, color='green',left=list(map(lambda g, y: g+y, others,carbohydrates)))\n",
    "plt.barh(y,fat, color='blue',left=list(map(lambda g, y: g+y, carbohydrates,sugars)))\n",
    "\n",
    "# labels\n",
    "y_labels=['Stevia','Farine bio de millet','Butter & Herb Mashed Potatoes','le pennette rigate N87','Enriched Macaroni Product, Fettucine','Premium jasmine Rice','Organic Sprouted Whole Wheat Flour','Thé glacé earl grey','Barbar au Miel','Chicorée']\n",
    "plt.yticks(y, y_labels)\n",
    "\n",
    "# legend\n",
    "plt.legend(['others','carbohydrates', 'sugars','fat'], loc='upper right')\n",
    "\n",
    "# set the title\n",
    "plt.title('Graph 4 - Top 10 most energetic products - Breakdown of core nutrients (% of total calories/100g)', fontweight=\"bold\")\n",
    "\n",
    "# invert y axis\n",
    "plt.gca().invert_yaxis() # invert the y axis to get the highest difficulty first\n",
    "plt.rcParams[\"figure.figsize\"]=[20,10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeways\n",
    "* There are 29 products in the foodfact database matching balanced nutrition criteria (Graph 3). Their energy level is below 200Kj per 100g\n",
    "* Most of them could be grouped in 3 categories (Graph 3): Natural product (e.g. Stevia), limited transformed products (flour, rice, pasta) or antioxidant beverages\n",
    "* If we focus on the Top 10 in terms of claories (among those 29 products), most of them are made of a majority of carbohydrates, a minority of fat and sugars, the difference coming from other constituents (Graph 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at \"ingredients_lits\" and transform data as strings\n",
    "df2['ingredients_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe to perform manipulation\n",
    "new_ingredients = df2[\"ingredients_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove all unwanted elements; i.e text between brackets and then replacing the \"space\" with a \", \"\n",
    "def transform(s):\n",
    "    s=s.str.replace(r'\\(.*?\\)', '') # text between brackets\n",
    "    s=s.str.replace('  ',', ') # 2 spaces replaced by a \", \"\n",
    "    s=s.str.replace('\\d*','') # replace '*' by a space\n",
    "    s=s.str.replace('%','')# replace % by a space\n",
    "    s=s.str.replace('*','') # replace * by a space\n",
    "    s=s.str.replace(' -',',')\n",
    "    s=s.str.replace('.','')\n",
    "    s=s.str.replace(' and',',')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the cleaning \"transform\" function to new_ingredients data\n",
    "new_ingredients=transform(new_ingredients)\n",
    "new_ingredients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column \"new_ingredients\"\n",
    "df2['new_ingredients']=new_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the former column \"ingredients_text\"\n",
    "df2.drop(['ingredients_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the list of ingredients into individual columns (+ make sure we have strings)\n",
    "df3=(df2['new_ingredients'].str.split(',', expand=True).rename(columns=lambda x: f\"ingredients_{x+1}\"))\n",
    "df3.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select most common group of igredients; i.e column 1 to 10\n",
    "df3a=df3[[\"ingredients_1\", \"ingredients_2\", \"ingredients_3\", \"ingredients_4\", \"ingredients_5\", \"ingredients_6\", \"ingredients_7\", \"ingredients_8\", \"ingredients_9\", \"ingredients_10\"]]\n",
    "df3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df3a.stack().reset_index(drop=True)\n",
    "df5=df4.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.value_counts()\n",
    "# we will group together English and French words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeaway (Question C)\n",
    "* 5 most common ingredients are: sugar, water, citric acid & corn syrup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Time Series\n",
    "* Analyze the mean time difference between these two values\n",
    "* Analyze the mean number of created items per month over the timeline of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataframe with key variables for this exercise: code, created_date, last_modified_date\n",
    "tsdf= df2[['code','created_datetime','last_modified_datetime']]\n",
    "tsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column to compute the time difference\n",
    "tsdf['time_diff']=(tsdf['last_modified_datetime'] - tsdf['created_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "tsdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the highest values\n",
    "tsdf.sort_values('time_diff',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf['time_diff'].mean()\n",
    "# the mean of the time difference is: 110 days 15:18:52.405461\n",
    "# it idincates that there are probably some products which are up-dated on a regular basis since the beginning of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "* Mean of the time difference is: 110 days 15:18:52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the datetime as index to compute the mean of items created by time period\n",
    "new=tsdf.set_index('created_datetime')\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fixed interval per month\n",
    "by_month =new.groupby(new.index.month).count()\n",
    "by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data per month\n",
    "by_month['code'].plot()\n",
    "\n",
    "# labels\n",
    "x=np.arange(12)+1\n",
    "x_labels=['Jan.','Feb.','March','April','May','June','July','August','Sep.','Oct.','Nov.','Dec.']\n",
    "plt.xticks(x, x_labels)\n",
    "\n",
    "# set the title\n",
    "plt.title('Average number of created products per month over the period 2012-2017', fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Build a database\n",
    "* restrict your data to 1000 entries and 5 columns of your choice\n",
    "* create a connection to a sqlite3 database\n",
    "* create one or multiple tables, at least one of the tables should have a PRIMARY KEY\n",
    "* fill the database with your data\n",
    "* run at least one query to demonstrate that it works correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe which will be used for filling the database\n",
    "dbase=add_analysis_clean[['product_name', 'energy_100g','additives_n','proteins_100g','new_countries']][0:1000]\n",
    "dbase.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a database\n",
    "db = sqlite3.connect('dbsubmission2.db')\n",
    "\n",
    "# defining our helper function for running queries\n",
    "def run_query(query):\n",
    "    return pd.read_sql_query(query,db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data into the database\n",
    "dbase.to_sql(name='dbsubmission2', con=db, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all data have been loaded\n",
    "#checking that all the data was loaded\n",
    "run_query(\"SELECT COUNT(*) FROM dbsubmission2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first 5 rows of the table \"dbsubmission2\"\n",
    "run_query(\"SELECT * FROM dbsubmission2 LIMIT 5;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: How many products with an energy level between 1200 and 2000?\n",
    "query='''\n",
    "SELECT energy_100g, COUNT(*) AS \"Count\" FROM dbsubmission2 \n",
    "WHERE (energy_100g BETWEEN 1200 AND 2000)\n",
    "limit 10;\n",
    "'''\n",
    "run_query(query) #314 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top 10 products with the highest level of energy and from which countries?\n",
    "query='''\n",
    "SELECT product_name, energy_100g, new_countries FROM dbsubmission2 \n",
    "ORDER BY energy_100g DESC \n",
    "LIMIT 10;\n",
    "'''\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Finding correlations\n",
    "* Identify the variables which most affect the nutritional score and provide some insight into which factors cause both a low or a high nutritional score\n",
    "\n",
    "    #### Preliminary remarks\n",
    "    * Nutrition score definition is available at: https://www.ndph.ox.ac.uk/cpnp/files/about/uk-ofcom-nutrient-profile-model.pdf\n",
    "    * Points for foods and drinks fall on a scale from 1 to 100 where 1 is the least healthy and 100 is the most healthy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ndph.ox.ac.uk/cpnp/files/about/uk-ofcom-nutrient-profile-model.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the datframe cleaned at the question A as a starting point\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of products having a nutrition score \n",
    "df2['nutrition-score-uk_100g'].isnull().sum() # 53,199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN for nutrition-score-uk-100g\n",
    "df2ns=df2.dropna(subset=['nutrition-score-uk_100g'])\n",
    "df2ns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2ns.describe()\n",
    "# Globally Mean and Standard deviation vary a lot from one variable to the other\n",
    "# We would need to normalize data before performing the correlatin analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the distribution plot\n",
    "sns.distplot(df2ns['nutrition-score-uk_100g'])\n",
    "plt.title('Displot nutrition score', fontweight=\"bold\")\n",
    "plt.rcParams[\"figure.figsize\"]=[20,10]\n",
    "# In order to focus the interpretation, we would consider 2 groups of products the \"less healthy\" having a NS from -10 to 10\n",
    "# and \" more healthy\" products with a nutrition score from 10 to 30\n",
    "# we may decide to discretize the NS if the normalization will not provide conclusive highlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the correlation matrix analysis we will consider the following criteria:\n",
    "* A correlation matrix analysis performed on all numeric variables did not provide relevant results. We will adopt a more targeted approach based on the following principles:\n",
    "    * Variables included in the calculation of the nutrition score (for which we have data): Energy, Saturated Fat, Total Sugar, Sodium, Fruit, Veg & Nuts (%), Fibre (g)\n",
    "    * Other variables which we would assume being potentially relevant based on nutrition reports: number of additives..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the overall nutrition score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables to perform the targeted correlation analysis\n",
    "targ_corr=df2ns[['product_name',           \n",
    " 'additives_n',\n",
    " 'ingredients_from_palm_oil_n',\n",
    " 'ingredients_that_may_be_from_palm_oil_n',\n",
    " 'energy_100g',\n",
    " 'energy-from-fat_100g',\n",
    " 'fat_100g',\n",
    "'saturated-fat_100g',\n",
    " 'carbohydrates_100g',\n",
    " 'sugars_100g',\n",
    "'fiber_100g',\n",
    " 'starch_100g',\n",
    " 'proteins_100g',\n",
    " 'salt_100g',\n",
    " 'sodium_100g',\n",
    " 'calcium_100g',\n",
    " 'fruits-vegetables-nuts_100g',\n",
    " 'fruits-vegetables-nuts-estimate_100g',\n",
    " 'collagen-meat-protein-ratio_100g',\n",
    " 'nutrition-score-uk_100g']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index to get the product names\n",
    "targ_corr.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values by 0\n",
    "targ_corr.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = targ_corr.corr()\n",
    "# source:https://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# define the style\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '200px', 'font-size': '10pt'})\\\n",
    "    .set_precision(2)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeways (overall nutrition score)\n",
    "* Overall nutrition score is positively correlated to energy_100g (0.56), fat_100g (0.59), saturated fate (0.64) and sugars_100g to a certain extent (0.42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the low nutrition score (from -10 to +10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will analyse correlation matrix for low nutrition score; between -10 and +10\n",
    "targ_corr_low= targ_corr[(targ_corr['nutrition-score-uk_100g']  >= 0) & (targ_corr['nutrition-score-uk_100g'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values by 0\n",
    "targ_corr_low.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = targ_corr_low.corr()\n",
    "# source:https://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# define the style\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_precision(2)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeways (low nutrition score)\n",
    "* From -10 to 10: correlation score are: energy 0.3, fat 0.37, sugar 0.36 - There are not very strong variables correlated to nutrition score\n",
    "* From 0 to 10: energy 0.54, fat 0.36, carbohydrates 0.45, sugars 0.34 - Energy and carbohydrates present the highest correlation indicator. However, not very conclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the high nutrition score (from 10 to 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will analyse correlation matrix for low nutrition score; between -10 and +10\n",
    "targ_corr_high= targ_corr[(targ_corr['nutrition-score-uk_100g']  > 10) & (targ_corr['nutrition-score-uk_100g'] <= 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values by 0\n",
    "targ_corr_high.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = targ_corr_high.corr()\n",
    "# source:https://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# define the style\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_precision(2)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeways (high nutrition score)\n",
    "* From >10 to 30: correlation score are: energy 0.27, fat 0.36, saturated fat 0.53 - saturated fat seems to present some correlation to nutrition score\n",
    "* From 20 to 30: carbohydrates 0.24, sugars 0.24 - There are not very strong variables correlated to nutrition score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-up analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation analysis with groups of food is not conclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_corr=df2ns[['product_name','nutrition-score-uk_100g','pnns_groups_1','pnns_groups_2']]\n",
    "cat_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index to get the product names\n",
    "cat_corr.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values by 0\n",
    "cat_corr.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe (dummy) including dummy variable from the original \"Region\" variable\n",
    "dummy1=pd.get_dummies(cat_corr['pnns_groups_1'])\n",
    "dummy2=pd.get_dummies(cat_corr['pnns_groups_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatanate cat_corr and dummy dataframes\n",
    "cat_corrA=pd.concat([cat_corr, dummy1, dummy2], axis=1)\n",
    "cat_corrA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop pnns_groups_ 1 variable\n",
    "cat_corr1=cat_corrA.drop('pnns_groups_1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop pnns_groups_ 2 variable\n",
    "cat_corr2=cat_corr1.drop('pnns_groups_2', axis=1)\n",
    "cat_corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_corr2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force correlation exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables to perform the correlation analysis\n",
    "df2ns_corr=df2ns[['product_name',\n",
    " 'additives_n',\n",
    " 'ingredients_from_palm_oil_n',\n",
    " 'ingredients_that_may_be_from_palm_oil_n',\n",
    " 'energy_100g',\n",
    " 'energy-from-fat_100g',\n",
    " 'fat_100g',\n",
    " 'saturated-fat_100g',\n",
    " '-caprylic-acid_100g',\n",
    " '-capric-acid_100g',\n",
    " '-lauric-acid_100g',\n",
    " '-myristic-acid_100g',\n",
    " '-palmitic-acid_100g',\n",
    " '-stearic-acid_100g',\n",
    " '-arachidic-acid_100g',\n",
    " '-behenic-acid_100g',\n",
    " '-montanic-acid_100g',\n",
    " 'monounsaturated-fat_100g',\n",
    " 'polyunsaturated-fat_100g',\n",
    " 'omega-3-fat_100g',\n",
    " '-alpha-linolenic-acid_100g',\n",
    " '-eicosapentaenoic-acid_100g',\n",
    " '-docosahexaenoic-acid_100g',\n",
    " 'omega-6-fat_100g',\n",
    " '-linoleic-acid_100g',\n",
    " '-arachidonic-acid_100g',\n",
    " '-gamma-linolenic-acid_100g',\n",
    " '-dihomo-gamma-linolenic-acid_100g',\n",
    " 'omega-9-fat_100g',\n",
    " '-oleic-acid_100g',\n",
    " '-gondoic-acid_100g',\n",
    " 'trans-fat_100g',\n",
    " 'cholesterol_100g',\n",
    " 'carbohydrates_100g',\n",
    " 'sugars_100g',\n",
    " '-sucrose_100g',\n",
    " '-glucose_100g',\n",
    " '-fructose_100g',\n",
    " '-lactose_100g',\n",
    " '-maltose_100g',\n",
    " '-maltodextrins_100g',\n",
    " 'starch_100g',\n",
    " 'polyols_100g',\n",
    " 'fiber_100g',\n",
    " 'proteins_100g',\n",
    " 'casein_100g',\n",
    " 'serum-proteins_100g',\n",
    " 'nucleotides_100g',\n",
    " 'salt_100g',\n",
    " 'sodium_100g',\n",
    " 'alcohol_100g',\n",
    " 'vitamin-a_100g',\n",
    " 'beta-carotene_100g',\n",
    " 'vitamin-d_100g',\n",
    " 'vitamin-e_100g',\n",
    " 'vitamin-k_100g',\n",
    " 'vitamin-c_100g',\n",
    " 'vitamin-b1_100g',\n",
    " 'vitamin-b2_100g',\n",
    " 'vitamin-pp_100g',\n",
    " 'vitamin-b6_100g',\n",
    " 'vitamin-b9_100g',\n",
    " 'folates_100g',\n",
    " 'vitamin-b12_100g',\n",
    " 'biotin_100g',\n",
    " 'pantothenic-acid_100g',\n",
    " 'silica_100g',\n",
    " 'bicarbonate_100g',\n",
    " 'potassium_100g',\n",
    " 'chloride_100g',\n",
    " 'calcium_100g',\n",
    " 'phosphorus_100g',\n",
    " 'iron_100g',\n",
    " 'magnesium_100g',\n",
    " 'zinc_100g',\n",
    " 'copper_100g',\n",
    " 'manganese_100g',\n",
    " 'fluoride_100g',\n",
    " 'selenium_100g',\n",
    " 'chromium_100g',\n",
    " 'molybdenum_100g',\n",
    " 'iodine_100g',\n",
    " 'caffeine_100g',\n",
    " 'taurine_100g',\n",
    " 'ph_100g',\n",
    " 'fruits-vegetables-nuts_100g',\n",
    " 'fruits-vegetables-nuts-estimate_100g',\n",
    " 'collagen-meat-protein-ratio_100g',\n",
    " 'cocoa_100g',\n",
    " 'carbon-footprint_100g',\n",
    " 'nutrition-score-uk_100g']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index to get the product names\n",
    "df2ns_corr.set_index('product_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values by 0\n",
    "df2ns_corr.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalizer\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Transform feature matrix\n",
    "normalizer.transform(df2ns_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df2ns_corr.corr()\n",
    "# source:https://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# define the style\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '200px', 'font-size': '10pt'})\\\n",
    "    .set_precision(2)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy 0.54, fat 0.59, saturated fat 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
